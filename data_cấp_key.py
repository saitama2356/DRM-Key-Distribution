# -*- coding: utf-8 -*-
"""Data Cấp key.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cu0xi6bEOZ1IR5ow44YBfzrcgljofMEp
"""

#NOTE : Chi Du Doan duoc so ngay tiep theo vi data con it

import pyodbc
import pandas as pd

server = '172.20.02.110'
database = 'RecommendDB'
username = 'hungnt84'
password = 'rm!t2003'
cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)
cursor = cnxn.cursor()

df = pd.read_sql_query('select * from PBI_Overview_BoxUsage', cnxn)
df

df["DateTime"] = pd.to_datetime(df["DateTime"])

df = df.drop(['Date'],1)

df = df.loc[df['Type'] == 'Daily']

df = df[df['DateTime'] < '2020-05-17']

df = df[df['DateTime'] >= '2020-04-03']

df = df.sort_values('DateTime', ascending = False)

df = df.drop(['Type'],1)

df

df.dtypes

import pandas as pd
import matplotlib as mplt
from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns

sql = """
-- # Get View BHD # --
With
ViewFilmBHD as
(select distinct A.CustomerID,A.Ftype, cast(A.Date as date) as Date, B.isDRM from Log_BHD_MovieID A
left join MV_PropertiesshowVN B on A.MovieID = B.Id
where isDRM = 1 and A.Date between '2019-05-01' and getdate()
group by A.MovieID,A.Ftype,A.Date,B.isDRM, A.CustomerID)

--select count(distinct A.Contract) as Number , Date, Ftype
--from ViewMovie A
--group by Date, Ftype
--order by date desc

--#Get View Film Plus # --

, ViewFilmPlus as (select
Distinct CustomerID,
C.FType,
cast(C.Date as Date) as Date,
B.IsDrm
from Log_Fimplus_MovieId C
left join MV_PropertiesshowVN B on C.MovieID = B.Id
where isDRM = 1 and C.Date between '2019-05-01' and getdate()
group by CustomerID, C.Date, B.IsDRM, C.FType )

--select count(distinct C.Contract) as Number , Date, Ftype
--from ViewMovie2 C
--group by Date, Ftype
--order by date desc

-- # Combine View # --
, ViewFilm as (Select distinct v1.CustomerID as CustomerID, Date , Ftype from ViewFilmBHD v1
group by v1.CustomerID, Date, Ftype
union
select distinct v2.CustomerID as Number, Date, Ftype  from ViewFilmPlus v2
group by V2.CustomerID, Date, Ftype
 )

-- Danh Sach K+ / Dac Sac
, CustomersList as
(Select CS.CustomerID,CS.ServiceID
from Customers C join CustomerService CS on c.ID = cs.CustomerID
where C.Status = 1 AND C.AccountType = 1 and (C.SubStatus is null or C.SubStatus =1 )
And c.BoxType = 4 AND (StopDate is null or Stopdate >= Getdate())
And C.ID < 10000000 and cs.ToDate  >=cs.Date AND CS.Enable=1 AND c.LocationID<1000 and C.MAC IS NOT NULL
and ServiceID in (60,89,148,149,150,154))

-- KeyKH Xem DS/K+
, TotalKH as
(select distinct customerID as CustomerID , cast(DateStamp as Date) as Date, MAC from Log_GetDRM_List
where CustomerID != 0
group by CustomerID,Cast(DateStamp as Date), MAC)

-- Mapping --
, Mapping as
(Select Distinct T.CustomerID ,T.MAC, T.Date, ServiceID
from TotalKH T
left join CustomersList L on T.CustomerID = L.CustomerID
where ServiceID is not null
group by T.CustomerID, T.MAC, T.Date, ServiceID )

-- Final Mapping --

, Final_Mapping as
(Select M.CustomerID, Mac, M.Date, ServiceID , F.FType
from Mapping M
left join ViewFilm F on M.CustomerID = F.CustomerID
group by M.CustomerID,M.Mac,M.Date,M.ServiceID,F.Ftype)

,Calculation as
(Select CustomerID, Date From Final_Mapping
Union all
Select CustomerID, Date from ViewFilm
where Date between '2020-4-3' and getdate()
group by CustomerID,Date )

--Lay so Total theo ngay --
Select Sum(Ca.Number) as Total, Ca.Date from
(select count(CustomerID) as Number,Date
From Calculation
Group by Date) Ca
group by Ca.Date
order by Ca.Date desc
"""
df2 = pd.read_sql_query(sql, cnxn)
df2

!pip install pyodbc
import pyodbc
import pandas as pd


# Database credentials
server = '172.20.02.110'
database = 'RecommendDB'
username = 'hungnt84'
password = 'rm!t2003'

# Establish database connection
cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)


sql = """
-- # Get View BHD # --
With
ViewFilmBHD as
(select distinct A.CustomerID,A.Ftype, cast(A.Date as date) as Date, B.isDRM from Log_BHD_MovieID A
left join MV_PropertiesshowVN B on A.MovieID = B.Id
where isDRM = 1 and A.Date between '2019-05-01' and getdate()
group by A.MovieID,A.Ftype,A.Date,B.isDRM, A.CustomerID)

--select count(distinct A.Contract) as Number , Date, Ftype
--from ViewMovie A
--group by Date, Ftype
--order by date desc

--#Get View Film Plus # --

, ViewFilm

df2["Date"] = pd.to_datetime(df2["Date"])

df2_new = df2[df2['Date'] < '2020-05-17']

df2_new = df2_new[df2_new['Date'] >= '2020-05-01']

df2_new.dtypes

df2_new

Y = df2_new['Total']
X = df2_new['Date']
plt.plot(X,Y)

indexed_df2_new = df2_new.set_index(['Date'])

df2_new

sns.lineplot(x="Date", y="Total", data=df2_new)

indexed_df2_new

plt.xlabel("Date")
plt.ylabel("TotalKey")
plt.plot(indexed_df2_new)

#Rolling Statistics
rolmean = indexed_df2_new.rolling(window=2).mean()
rolstd = indexed_df2_new.rolling(window=2).std()
print(rolmean,rolstd)

original = plt.plot(indexed_df2_new, color='blue', label='Original')
mean = plt.plot(rolmean,color ='red',label='RollingMean')
std = plt.plot(rolstd, color ='black', label ='RollingStdev')
plt.legend(loc='best')
plt.title('Rolling Mean & Standard Deviation')
plt.show(block=False)

#Dickey-Fuller test ( Stationery Test)
from statsmodels.tsa.stattools import adfuller
print('Results of Dickey-FUller Test:')
dftest = adfuller(indexed_df2_new['Total'],autolag='AIC')
dfoutput = pd.Series(dftest[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value

print(dfoutput)

#estimating trend
indexed_df2_new_logScale = np.log(indexed_df2_new)
plt.plot(indexed_df2_new_logScale)

movingAverage= indexed_df2_new_logScale.rolling(window=2).mean()
movingSTD = indexed_df2_new_logScale.rolling(window=2).std()
plt.plot(indexed_df2_new_logScale)
plt.plot(movingAverage, color ='red')

dfLogScaleMinusMA = indexed_df2_new_logScale - movingAverage
dfLogScaleMinusMA.head()
#RemoveNA
dfLogScaleMinusMA.dropna(inplace=True)
dfLogScaleMinusMA

from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries):
    #Determining rolling statistics
    movingAverage = timeseries.rolling(window=2).mean()
    movingSTD =timeseries.rolling(window=2).std()

    #Plot rolling statistics:
    orig =plt.plot(timeseries, color ='green',label='Original')
    mean =plt.plot(movingAverage,color='red',label='Rolling Mean')
    std =plt.plot(movingSTD, color='black',label='Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

    #Perform Dickey-Fuller Test:
    print('Results of DIckey-Fuller Test:')
    dftest = adfuller(timeseries['Total'],autolag='AIC')
    dfoutput = pd.Series(dftest[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print(dfoutput)

test_stationarity(dfLogScaleMinusMA)

exponentialDecayWeightedAverage = indexed_df2_new_logScale.ewm(halflife=3,min_periods=0,adjust=True).mean()
plt.plot(indexed_df2_new_logScale)
plt.plot(exponentialDecayWeightedAverage,color='red')

dfLogScaleMinusMEDA = indexed_df2_new_logScale - exponentialDecayWeightedAverage
test_stationarity(dfLogScaleMinusMEDA)

dfLogDiffShifting = indexed_df2_new_logScale - indexed_df2_new_logScale.shift()
plt.plot(indexed_df2_new_logScale)

dfLogDiffShifting.dropna(inplace=True)
test_stationarity(dfLogDiffShifting)

from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(indexed_df2_new_logScale)
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
plt.subplot(411)
plt.plot(indexed_df2_new_logScale,label='Original')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(trend,label='Trend')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(seasonal,label='Seasonality')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(residual,label='Residuals')
plt.legend(loc='best')
plt.tight_layout()

decomposedLogData = seasonal
decomposedLogData.dropna(inplace=True)
test_stationarity(decomposedLogData)

from statsmodels.tsa.arima_model import ARIMA

#AR MODEL
model = ARIMA(indexed_df2_new_logScale,order=(0,1,1))
result_AR=model.fit(disp=-1)
plt.plot(dfLogDiffShifting)
plt.plot(result_AR.fittedvalues, color='red')
plt.legend(loc='best')
plt.title('RSS: %.4f'% sum((result_AR.fittedvalues-dfLogDiffShifting['Total'])**2))
print('Plotting AR Model')

#AR MODEL
model = ARIMA(indexed_df2_new_logScale,order=(0,1,2))
result_AR=model.fit(disp=-1)
plt.plot(dfLogDiffShifting)
plt.plot(result_AR.fittedvalues, color='red')
plt.legend(loc='best')
plt.title('RSS: %.4f'% sum((result_AR.fittedvalues-dfLogDiffShifting['Total'])**2))
print('Plotting AR Model')

#AR MODEL
model = ARIMA(indexed_df2_new_logScale,order=(0,1,3))
result_AR = model.fit(disp=-1)
plt.plot(dfLogDiffShifting)
plt.plot(result_AR.fittedvalues, color='red')
plt.title('RSS: %.4f'% sum((result_AR.fittedvalues-dfLogDiffShifting['Total'])**2))
print('Plotting AR Model')

#Choose the best AR Model based on RSS
model = ARIMA(indexed_df2_new_logScale,order=(0,1,1))
result_AR=model.fit(disp=-1)
plt.plot(dfLogDiffShifting)
plt.plot(result_AR.fittedvalues, color='red')
plt.legend(loc='best')
plt.title('RSS: %.4f'% sum((result_AR.fittedvalues-dfLogDiffShifting['Total'])**2))
print('Plotting AR Model')

predictions_ARIMA_diff = pd.Series(result_AR.fittedvalues,copy=True)
print(predictions_ARIMA_diff.head())

#convert to cumulative sum
predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()
print(predictions_ARIMA_diff_cumsum.head())

predictions_ARIMA_log =pd.Series(indexed_df2_new_logScale['Total'],index = indexed_df2_new_logScale.index)
predictions_ARIMA_log =predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)
predictions_ARIMA_log.head()

predictions_ARIMA =np.exp(predictions_ARIMA_log)
plt.plot(indexed_df2_new)
plt.plot(predictions_ARIMA)

np.exp(indexed_df2_new_logScale)

#prediction
result_AR.plot_predict(1,40)
x = result_AR.forecast(steps=7)

result_AR.forecast(steps=7)

todf = np.exp(x[0])
dudoan=pd.DataFrame(data=todf)
dudoan

#df.to_excel("output.xlsx")

#df_new = df_new.drop(['Date'],1)

#df_new.head(5)

#df_new.dtypes

df_merge = df.merge(df2,left_on='DateTime',right_on='Date')

df_merge = df_merge.drop(['DateTime'],1)

df_merge = df_merge.drop(['Date'],1)

df_merge.head()

df_merge.to_csv("data_outputTime")

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
from sklearn.linear_model import LinearRegression
# %matplotlib inline

scale = StandardScaler()
X= df_merge[['TotalActive','TotalActiveBox']]
Y = df_merge[['Total']]

scaled_DF = scale.fit_transform(df_merge)

scaledX = scale.fit_transform(X)

scaledY = scale.fit_transform(Y)

scaled_DF

array_to_df = pd.DataFrame(scaled_DF)

array_to_df

array_to_df = array_to_df.rename(columns={0: 'TotalActive', 1: 'TotalActiveBox',2:'Total'})

array_to_df

array_to_df.corr()['Total'].sort_values()

sns.boxplot(x= 'TotalActive', y ='Total', data = array_to_df)

sns.boxplot(x='TotalActiveBox', y ='Total', data = array_to_df)

#sns.regplot(x='TotalActive', y='Total Key', data=df_new)

#sns.regplot(x='TotalActiveBox', y='Total Key', data=df_new)

## Correlation Calculation #

#df_new.corr()['Total Key'].sort_values()

#X = df_new[['TotalActive']]
#Y = df_new['Total Key']
#lm = LinearRegression()
#lm.fit(X,Y)
#lm.score(X, Y)

#X2 = df_new[['TotalActiveBox']]
#Y2 = df_new['Total Key']
#lm = LinearRegression()
#lm.fit(X2,Y2)
#lm.score(X2,Y2)

#features = ["TotalActiveBox","TotalActive"]

#X = df_new[features]
#Y = df_new['Total Key']
#lm = LinearRegression()
#lm.fit(X,Y)
#lm.score(X,Y)

#lm.fit(X,Y)

#df = pd.read_excel('/home/maxtran/Desktop/Data Cấp K.xlsx')

#df

#df = df.drop(['TotalActive'],1)

#df = df.drop(['TotalActiveBox'],1)

#df.dtypes

#df.describe